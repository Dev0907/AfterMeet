from config.settings import Config

class ChatInterface:
    """Interactive chat interface for meeting analysis"""
    
    def __init__(self, api_clients, vector_store):
        self.api_clients = api_clients  # Changed to use api_clients instead of just cohere_client
        self.vector_store = vector_store
    
    def is_meeting_related_question(self, question, meeting_data):
        """
        Validate if the question is related to the meeting.
        Returns: (is_related: bool, reason: str)
        """
        question_lower = question.lower()
        
        meeting_keywords = [
            'meeting', 'discuss', 'said', 'mention', 'talk', 'speaker', 'participant',
            'action', 'task', 'deadline', 'decision', 'summary', 'topic', 'agenda',
            'who', 'what', 'when', 'why', 'how', 'which', 'tell', 'explain', 'describe',
            'sentiment', 'mood', 'tone', 'owner', 'assigned', 'responsible', 'urgent',
            'important', 'critical', 'priority', 'risk', 'issue', 'problem', 'solution',
            'agree', 'disagree', 'point', 'highlight', 'key', 'main', 'focus', 'outcome',
            'result', 'conclusion', 'next', 'step', 'follow', 'up', 'plan', 'schedule'
        ]
        
        if any(keyword in question_lower for keyword in meeting_keywords):
            return True, "Question contains meeting-related keywords"
        
        speakers = list(meeting_data.get('speakers', {}).keys())
        if any(speaker.lower() in question_lower for speaker in speakers):
            return True, "Question mentions a meeting participant"
        
        topics = meeting_data.get('summary', {}).get('topics_discussed', [])
        if not topics:
            topics = meeting_data.get('topics', [])
        if any(topic.lower() in question_lower for topic in topics if topic):
            return True, "Question mentions a meeting topic"
        
        return True, "Allowing question by default"
    
    def chat_with_transcript(self, user_question, processed_data, use_semantic_search=True):
        """
        Answer user questions based on meeting data using LLM with semantic search
        Only answers questions related to the meeting.
        """
        # Validate if question is meeting-related
        is_related, validation_reason = self.is_meeting_related_question(user_question, processed_data)
        
        if not is_related:
            return f"I can only answer questions related to this meeting. Your question appears to be off-topic. Please ask something about the meeting content, participants, decisions, or action items."
        
        transcript = processed_data.get('transcript', [])
        summary = processed_data.get('summary', {})
        meeting_id = processed_data.get('meeting_id')

        # Use semantic search to find most relevant parts
        relevant_context = ""
        if use_semantic_search and meeting_id:
            try:
                search_results = self.vector_store.search_relevant_transcript(user_question, meeting_id, top_k=5)
                if search_results:
                    relevant_context = "\n\nMOST RELEVANT TRANSCRIPT SECTIONS (Semantic Search):\n"
                    for i, result in enumerate(search_results, 1):
                        payload = result.payload
                        relevant_context += f"{i}. [{payload['timestamp']}] {payload['speaker_name']}: {payload['text']} (Relevance: {result.score:.3f})\n"
            except Exception as e:
                print(f"Semantic search warning: {e}")

        # Format full transcript with sentiment
        formatted_transcript = "\n".join(
            f"[{t['timestamp']}] {t['speaker_name']} (Sentiment: {t['sentiment']}): {t['text']}"
            for t in transcript
        )

        # Format summary
        formatted_summary = f"""
EXECUTIVE SUMMARY:
{summary.get('executive_summary', 'N/A')}

ACTION ITEMS:
"""
        for i, item in enumerate(summary.get('action_items', []), 1):
            formatted_summary += f"""
{i}. Task: {item.get('task', 'N/A')}
   Owner: {item.get('owner', 'N/A')}
   Deadline: {item.get('deadline', 'N/A')}
   Urgency: {item.get('urgency', 'N/A')}
   Reason: {item.get('urgency_reason', 'N/A')}
"""

        formatted_summary += f"""
TOPICS DISCUSSED: {', '.join(summary.get('topics_discussed', ['N/A']))}
NAMED ENTITIES: {', '.join(summary.get('named_entities', ['N/A']))}
OVERALL SENTIMENT: {summary.get('overall_sentiment', 'N/A')}
"""

        system_prompt = """You are an AI meeting assistant. Answer questions based ONLY on the provided meeting data.
Be concise and accurate. If information is not available in the meeting, say so clearly.
Reference specific timestamps and speakers when relevant.
Prioritize information from the most relevant sections when available.
Do not make up information that is not in the meeting."""

        user_message = f"""
FULL MEETING TRANSCRIPT:
{formatted_transcript}

MEETING SUMMARY:
{formatted_summary}
{relevant_context}

USER QUESTION: {user_question}

Provide a clear, concise answer based on the meeting data above. Use the relevant sections highlighted by semantic search for better context.
If the answer is not available in the meeting data, clearly state that."""

        # Use Ollama for Q&A to avoid Cohere rate limits
        try:
            response = self.api_clients.chat_with_ollama(
                prompt=user_message,
                system_prompt=system_prompt,
                temperature=0.3,
                max_tokens=500
            )
            return response
        except Exception as e:
            print(f"‚ö† Ollama failed, falling back to Cohere: {e}")
            # Fallback to Cohere if Ollama fails
            response = self.api_clients.cohere_client.chat(
                model="command-r-v2",
                preamble=system_prompt,
                message=user_message,
                temperature=0.3,
                max_tokens=500
            )
            return response.text
    
    def interactive_chat(self, processed_data):
        """Start an interactive chat session with semantic search"""
        print("\n" + "="*60)
        print("ü§ñ INTERACTIVE MEETING ASSISTANT")
        print("="*60)
        print("Ask questions about the meeting using natural language.")
        print("‚ú® Powered by Qdrant Cloud + Sentence Transformers + Ollama (qwen2:1.5b)")
        print("üìù Meeting analysis by Cohere, Q&A by Ollama (no rate limits!)")
        print("\nüìã Available Commands:")
        print("  ‚Ä¢ 'exit', 'quit', 'bye' - End chat session")
        print("  ‚Ä¢ 'summary' - Quick meeting overview")
        print("  ‚Ä¢ 'search <query>' - Semantic search in transcript")
        print("  ‚Ä¢ 'stats' - Qdrant storage statistics")
        print("  ‚Ä¢ 'actions' - List all action items with urgency")
        print("="*60 + "\n")

        while True:
            try:
                user_input = input("You: ").strip()

                if not user_input:
                    continue

                if user_input.lower() in ['exit', 'quit', 'bye']:
                    print("\n‚úì Chat session ended. Thank you! üëã")
                    break

                # Quick summary command
                if user_input.lower() == 'summary':
                    self._show_summary(processed_data)
                    continue

                # Actions command
                if user_input.lower() == 'actions':
                    self._show_actions(processed_data)
                    continue

                # Stats command
                if user_input.lower() == 'stats':
                    self._show_stats(processed_data)
                    continue

                # Semantic search command
                if user_input.lower().startswith('search '):
                    query = user_input[7:].strip()
                    if query:
                        self._perform_search(query, processed_data)
                    continue

                # Regular question - use LLM with semantic search
                print(f"\nüîç Analyzing your question...\n")
                response = self.chat_with_transcript(user_input, processed_data, use_semantic_search=True)
                print(f"ü§ñ Assistant: {response}\n")

            except KeyboardInterrupt:
                print("\n\n‚úì Chat interrupted. Goodbye! üëã")
                break
            except Exception as e:
                print(f"\n‚ö† Error: {e}")
                print("Please try again or type 'exit' to quit.\n")
    
    def _show_summary(self, processed_data):
        """Show quick meeting summary"""
        print(f"\nü§ñ Assistant: Here's your meeting overview:")
        print(f"\nüìù Executive Summary:")
        print(f"   {processed_data['summary'].get('executive_summary', 'N/A')}")
        print(f"\nüìä Statistics:")
        print(f"   ‚Ä¢ Action Items: {len(processed_data['summary'].get('action_items', []))}")
        print(f"   ‚Ä¢ Overall Sentiment: {processed_data['overall_sentiment']}")
        print(f"   ‚Ä¢ Topics: {', '.join(processed_data['summary'].get('topics_discussed', ['N/A']))}")
        print(f"   ‚Ä¢ Participants: {', '.join(processed_data['summary'].get('named_entities', ['N/A']))}\n")
    
    def _show_actions(self, processed_data):
        """Show all action items"""
        print(f"\nü§ñ Assistant: Here are all action items:\n")
        for i, item in enumerate(processed_data['summary'].get('action_items', []), 1):
            urgency_emoji = {
                'critical': 'üî¥',
                'high': 'üü†',
                'medium': 'üü°',
                'low': 'üü¢'
            }.get(item.get('urgency', 'medium'), '‚ö™')

            print(f"{i}. {urgency_emoji} [{item.get('urgency', 'N/A').upper()}] {item.get('task', 'N/A')}")
            print(f"   üë§ Owner: {item.get('owner', 'N/A')}")
            print(f"   üìÖ Deadline: {item.get('deadline', 'Not specified')}")
            print(f"   üí° Reason: {item.get('urgency_reason', 'N/A')}\n")
    
    def _show_stats(self, processed_data):
        """Show Qdrant statistics"""
        try:
            collection_info = self.vector_store.qdrant_client.get_collection(Config.COLLECTION_NAME)
            print(f"\nü§ñ Assistant: Qdrant Cloud Storage Statistics:")
            print(f"   ‚Ä¢ Collection: {Config.COLLECTION_NAME}")
            print(f"   ‚Ä¢ Total vectors: {collection_info.points_count}")
            print(f"   ‚Ä¢ Vector dimension: {collection_info.config.params.vectors.size}")
            print(f"   ‚Ä¢ Distance metric: {collection_info.config.params.vectors.distance}")
            print(f"   ‚Ä¢ Current Meeting ID: {processed_data['meeting_id']}")
            print(f"   ‚Ä¢ Embedding Model: {Config.EMBEDDING_MODEL}\n")
        except Exception as e:
            print(f"\n‚ö† Error fetching stats: {e}\n")
    
    def _perform_search(self, query, processed_data):
        """Perform semantic search"""
        print(f"\nüîç Searching for: '{query}'...")
        try:
            results = self.vector_store.search_relevant_transcript(query, processed_data['meeting_id'], top_k=3)
            
            if not results:
                print(f"ü§ñ Assistant: No relevant results found for '{query}'. Try a different search term.\n")
                return
                
            print(f"ü§ñ Assistant: Top {len(results)} most relevant transcript sections:\n")
            for i, result in enumerate(results, 1):
                payload = result.payload
                print(f"{i}. [{payload['timestamp']}] üí¨ {payload['speaker_name']}:")
                print(f"   \"{payload['text']}\"")
                print(f"   üìä Relevance Score: {result.score:.3f}\n")
        except Exception as e:
            print(f"‚ö† Error performing search: {e}")
            print("Please try again or type 'exit' to quit.\n")
    
    def interactive_chat(self, processed_data):
        """Start an interactive chat session with semantic search"""
        print("\n" + "="*60)
        print("ü§ñ INTERACTIVE MEETING ASSISTANT")
        print("="*60)
        print("Ask questions about the meeting using natural language.")
        print("‚ú® Powered by Qdrant Cloud + Sentence Transformers + Ollama (qwen2:1.5b)")
        print("üìù Meeting analysis by Cohere, Q&A by Ollama (no rate limits!)")
        print("\nüìã Available Commands:")
        print("  ‚Ä¢ 'exit', 'quit', 'bye' - End chat session")
        print("  ‚Ä¢ 'summary' - Quick meeting overview")
        print("  ‚Ä¢ 'search <query>' - Semantic search in transcript")
        print("  ‚Ä¢ 'stats' - Qdrant storage statistics")
        print("  ‚Ä¢ 'actions' - List all action items with urgency")
        print("="*60 + "\n")

        while True:
            try:
                user_input = input("You: ").strip()

                if not user_input:
                    continue

                if user_input.lower() in ['exit', 'quit', 'bye']:
                    print("\n‚úì Chat session ended. Thank you! üëã")
                    break

                # Quick summary command
                if user_input.lower() == 'summary':
                    self._show_summary(processed_data)
                    continue

                # Actions command
                if user_input.lower() == 'actions':
                    self._show_actions(processed_data)
                    continue

                # Stats command
                if user_input.lower() == 'stats':
                    self._show_stats(processed_data)
                    continue

                # Semantic search command
                if user_input.lower().startswith('search '):
                    query = user_input[7:].strip()
                    if query:
                        self._perform_search(query, processed_data)
                    continue

                # Regular question - use LLM with semantic search
                print(f"\nüîç Analyzing your question...\n")
                response = self.chat_with_transcript(user_input, processed_data, use_semantic_search=True)
                print(f"ü§ñ Assistant: {response}\n")

            except KeyboardInterrupt:
                print("\n\n‚úì Chat interrupted. Goodbye! üëã")
                break
            except Exception as e:
                print(f"\n‚ö† Error: {e}")
                print("Please try again or type 'exit' to quit.\n")
    
    def _show_summary(self, processed_data):
        """Show quick meeting summary"""
        print(f"\nü§ñ Assistant: Here's your meeting overview:")
        print(f"\nüìù Executive Summary:")
        print(f"   {processed_data['summary'].get('executive_summary', 'N/A')}")
        print(f"\nüìä Statistics:")
        print(f"   ‚Ä¢ Action Items: {len(processed_data['summary'].get('action_items', []))}")
        print(f"   ‚Ä¢ Overall Sentiment: {processed_data['summary'].get('overall_sentiment', 'N/A')}")
        print(f"   ‚Ä¢ Topics: {', '.join(processed_data['summary'].get('topics_discussed', ['N/A']))}")
        print(f"   ‚Ä¢ Participants: {', '.join(processed_data['summary'].get('named_entities', ['N/A']))}\n")
    
    def _show_actions(self, processed_data):
        """Show all action items"""
        print(f"\nü§ñ Assistant: Here are all action items:\n")
        for i, item in enumerate(processed_data['summary'].get('action_items', []), 1):
            urgency_emoji = {
                'critical': 'üî¥',
                'high': 'üü†',
                'medium': 'üü°',
                'low': 'üü¢'
            }.get(item.get('urgency', 'medium'), '‚ö™')

            print(f"{i}. {urgency_emoji} [{item.get('urgency', 'N/A').upper()}] {item.get('task', 'N/A')}")
            print(f"   üë§ Owner: {item.get('owner', 'N/A')}")
            print(f"   üìÖ Deadline: {item.get('deadline', 'Not specified')}")
            print(f"   üí° Reason: {item.get('urgency_reason', 'N/A')}\n")
    
    def _show_stats(self, processed_data):
        """Show Qdrant statistics"""
        try:
            collection_info = self.vector_store.qdrant_client.get_collection(Config.COLLECTION_NAME)
            print(f"\nü§ñ Assistant: Qdrant Cloud Storage Statistics:")
            print(f"   ‚Ä¢ Collection: {Config.COLLECTION_NAME}")
            print(f"   ‚Ä¢ Total vectors: {collection_info.points_count}")
            print(f"   ‚Ä¢ Vector dimension: {collection_info.config.params.vectors.size}")
            print(f"   ‚Ä¢ Distance metric: {collection_info.config.params.vectors.distance}")
            print(f"   ‚Ä¢ Current Meeting ID: {processed_data['meeting_id']}")
            print(f"   ‚Ä¢ Embedding Model: {Config.EMBEDDING_MODEL}\n")
        except Exception as e:
            print(f"\n‚ö† Error fetching stats: {e}\n")
    
    def _perform_search(self, query, processed_data):
        """Perform semantic search"""
        print(f"\nüîç Searching for: '{query}'...")
        try:
            results = self.vector_store.search_relevant_transcript(query, processed_data['meeting_id'], top_k=3)
            
            if not results:
                print(f"ü§ñ Assistant: No relevant results found for '{query}'. Try a different search term.\n")
                return
                
            print(f"ü§ñ Assistant: Top {len(results)} most relevant transcript sections:\n")
            for i, result in enumerate(results, 1):
                payload = result.payload
                print(f"{i}. [{payload['timestamp']}] üí¨ {payload['speaker_name']}:")
                print(f"   \"{payload['text']}\"")
                print(f"   üìä Relevance Score: {result.score:.3f}\n")
        except Exception as e:
            print(f"‚ö† Error performing search: {e}")
            print("Please try again or type 'exit' to quit.\n")
